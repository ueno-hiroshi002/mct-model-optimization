{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a91956",
   "metadata": {},
   "source": [
    "# Model Compression Toolkit (MCT) Wrapper API (PyTorch)\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/SonySemiconductorSolutions/mct-model-optimization/blob/main/tutorials/notebooks/mct_features_notebooks/pytorch/example_pytorch_mct_wrapper.ipynb)\n",
    "\n",
    "## Overview \n",
    "In this notebook, we provide a detailed explanation of the MCTWrapper class from the Model Compression Toolkit (MCT).\n",
    "Using this class enables a consistent implementation, making it easy to compare various quantization methods.\n",
    "In this tutorial, we take MobileNetV2 as an example and use MCTWrapper to apply the following quantization techniques:\n",
    "PTQ (Post-Training Quantization), PTQ with Mixed Precision, GPTQ (Gradient-based PTQ), GPTQ with Mixed Precision.\n",
    "By working through these methods, you will experience the convenience and flexibility of MCTWrapper, \n",
    "helping you to select the optimal quantization approach for your application.\n",
    "\n",
    "## Summary\n",
    "- **Setup**: Import required libraries and configure MCT with MobileNetV2 model\n",
    "- **Dataset Preparation**: Load and prepare ImageNet validation dataset with representative data generation\n",
    "- **Model Quantization using MCTWrapper**: Quantize the float model using MCTWrapper with four methods\n",
    "  - **PTQ**: Perform PTQ\n",
    "  - **PTQ + Mixed Precision**: Assign optimal quantization bit-width to each layer based on PTQ\n",
    "  - **GPTQ**: Perform GPTQ\n",
    "  - **GPTQ + Mixed Precision**: Assign optimal quantization bit-width to each layer based on GPTQ\n",
    "- **Evaluation**: Evaluate accuracy of all quantization methods\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852cf568",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q onnx==1.17.0\n",
    "!pip install -q torch==2.6.0 torchvision==0.21.0\n",
    "!pip install -q tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0b00c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "    !pip install model_compression_toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9da27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torchvision.datasets import ImageNet\n",
    "import model_compression_toolkit as mct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3ca588",
   "metadata": {},
   "source": [
    "Load a pre-trained MobileNetV2 model from torchvision, in 32-bits floating-point precision format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1d1be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = MobileNet_V2_Weights.IMAGENET1K_V2\n",
    "\n",
    "float_model = mobilenet_v2(weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b43e6",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "### Download ImageNet validation set\n",
    "Download ImageNet dataset (validation split only).\n",
    "\n",
    "This step may take several minutes...\n",
    "\n",
    "**Note:** For demonstration purposes, we use the validation set for the model quantization routines. Usually, a subset of the training dataset is used, but loading it is a heavy procedure that is unnecessary for the sake of this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9566bb3e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir('imagenet'):\n",
    "    !mkdir imagenet\n",
    "    !wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz\n",
    "    !wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfefae6",
   "metadata": {},
   "source": [
    "Extract ImageNet validation dataset using torchvision \"datasets\" module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ccaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageNet(root='./imagenet', split='val', transform=weights.transforms())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7b3369",
   "metadata": {},
   "source": [
    "## Representative Dataset\n",
    "For quantization with MCT, we need to define a representative dataset. This dataset is a generator that returns a list of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62009b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "n_iter = 10\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    for _ in range(n_iter):\n",
    "        yield [next(dataloader_iter)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0ee68",
   "metadata": {},
   "source": [
    "## Model Quantization using MCTWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb5d39",
   "metadata": {},
   "source": [
    "We implement quantizing example using MCTWrapper with four methods.\n",
    "\n",
    "By specifying the SDSP converter version, you can select the optimal quantization settings for IMX500.\n",
    "Here, we use the settings for SDSP Converter 3.14. For other settings, please see [here](https://github.com/SonySemiconductorSolutions/mct-model-optimization/tree/main/model_compression_toolkit/target_platform_capabilities).\n",
    "\n",
    "**Note:** This tutorial sets the minimum parameters required to run MCTWrapper. For details on omitted parameters, refer to [MCT Documentation](https://sonysemiconductorsolutions.github.io/mct-model-optimization/api/api_docs/classes/Wrapper.html#ug-wrapper).\n",
    "\n",
    "**Note:** This tutorial uses parameters focused on shorter run time for demonstration, resulting in lower accuracy. For improve accuracy, refer to other tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e556d1d9",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run PTQ with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe46712",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def PTQ_Pytorch(float_model: torch.nn.Module) -> Tuple[bool, torch.nn.Module]:\n",
    "    \"\"\"\n",
    "    Perform PTQ on PyTorch model.\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point PyTorch model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    framework = 'pytorch'               # Target framework (PyTorch)\n",
    "    method = 'PTQ'                      # Quantization method\n",
    "    use_mixed_precision = False         # Disable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration\n",
    "    param_items = [\n",
    "        ['sdsp_version', '3.14'],                           # Version of the SDSP converter\n",
    "        ['save_model_path', './qmodel_PTQ_Pytorch.onnx']    # Path to save quantized model as ONNX format\n",
    "    ]\n",
    "\n",
    "    # Execute quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model=float_model, \n",
    "        representative_dataset=representative_dataset_gen, \n",
    "        framework=framework, \n",
    "        method=method, \n",
    "        use_mixed_precision=use_mixed_precision, \n",
    "        param_items=param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6df571",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run PTQ + Mixed Precision with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8227d00b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def PTQ_Pytorch_mixed_precision(float_model: torch.nn.Module) -> Tuple[bool, torch.nn.Module]:\n",
    "    \"\"\"\n",
    "    Perform PTQ with Mixed Precision on PyTorch model.\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point PyTorch model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    framework = 'pytorch'               # Target framework (PyTorch)\n",
    "    method = 'PTQ'                      # Quantization method\n",
    "    use_mixed_precision = True          # Enable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration\n",
    "    param_items = [\n",
    "        ['sdsp_version', '3.14'],                                         # Version of the SDSP converter\n",
    "        ['num_of_images', 5],                                             # Number of images for Mixed-Precision calibration\n",
    "        ['weights_compression_ratio', 0.5],                               # Compression ratio of weights for Mixed-Precision\n",
    "        ['save_model_path', './qmodel_PTQ_Pytorch_mixed_precision.onnx']  # Path to save quantized model as ONNX format\n",
    "    ]\n",
    "\n",
    "    # Execute quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model=float_model, \n",
    "        representative_dataset=representative_dataset_gen, \n",
    "        framework=framework, \n",
    "        method=method, \n",
    "        use_mixed_precision=use_mixed_precision, \n",
    "        param_items=param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04d6a40",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run GPTQ with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c9da67",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def GPTQ_Pytorch(float_model: torch.nn.Module) -> Tuple[bool, torch.nn.Module]:\n",
    "    \"\"\"\n",
    "    Perform GPTQ on PyTorch model.\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point PyTorch model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    framework = 'pytorch'               # Target framework (PyTorch)\n",
    "    method = 'GPTQ'                     # Quantization method\n",
    "    use_mixed_precision = False         # Disable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration\n",
    "    param_items = [\n",
    "        ['sdsp_version', '3.14'],                          # Version of the SDSP converter\n",
    "        ['n_epochs', 5],                                   # Number of epochs for GPTQ optimization\n",
    "        ['save_model_path', './qmodel_GPTQ_Pytorch.onnx']  # Path to save quantized model as ONNX format\n",
    "    ]\n",
    "\n",
    "    # Execute quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model=float_model, \n",
    "        representative_dataset=representative_dataset_gen, \n",
    "        framework=framework, \n",
    "        method=method, \n",
    "        use_mixed_precision=use_mixed_precision, \n",
    "        param_items=param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e757e27",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run GPTQ + Mixed Precision with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2005ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPTQ_Pytorch_mixed_precision(float_model: torch.nn.Module) -> Tuple[bool, torch.nn.Module]:\n",
    "    \"\"\"\n",
    "    Perform GPTQ with Mixed Precision on PyTorch model.\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point PyTorch model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    framework = 'pytorch'               # Target framework (PyTorch)\n",
    "    method = 'GPTQ'                     # Quantization method\n",
    "    use_mixed_precision = True          # Enable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration\n",
    "    param_items = [\n",
    "        ['sdsp_version', '3.14'],                                          # Version of the SDSP converter\n",
    "        ['n_epochs', 5],                                                   # Number of epochs for GPTQ optimization\n",
    "        ['num_of_images', 5],                                              # Number of images for Mixed-Precision calibration\n",
    "        ['weights_compression_ratio', 0.5],                                # Compression ratio of weights for Mixed-Precision\n",
    "        ['save_model_path', './qmodel_GPTQ_Pytorch_mixed_precision.onnx']  # Path to save quantized model as ONNX format\n",
    "    ]\n",
    "\n",
    "    # Execute quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model=float_model, \n",
    "        representative_dataset=representative_dataset_gen, \n",
    "        framework=framework, \n",
    "        method=method, \n",
    "        use_mixed_precision=use_mixed_precision, \n",
    "        param_items=param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07704042",
   "metadata": {},
   "source": [
    "### Run Quantization\n",
    "Lastly, we quantize our model using MCTWrapper API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b29a22-dbbd-4aac-aba7-d3281324d31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic PTQ\n",
    "flag, quantized_model_ptq = PTQ_Pytorch(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e47383-d512-47bb-8602-bbc284f3786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PTQ with Mixed Precision\n",
    "flag, quantized_model_ptq_mixed_precision = PTQ_Pytorch_mixed_precision(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f3cd87-dde5-4463-8e84-1b3b11e447e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPTQ\n",
    "flag, quantized_model_gptq = GPTQ_Pytorch(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb3322-47d4-4fcd-8e6c-77037206d7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPTQ with Mixed Precision\n",
    "flag, quantized_model_gptq_mixed_precision = GPTQ_Pytorch_mixed_precision(float_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2172853",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Define a comprehensive evaluation function for PyTorch models that provides accurate performance measurement on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93cf436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: torch.nn.Module, testloader: DataLoader, mode: str) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate PyTorch model accuracy using a DataLoader.\n",
    "    \n",
    "    This function performs complete accuracy evaluation by:\n",
    "    - Moving model and data to available device (GPU/CPU)\n",
    "    - Running inference in evaluation mode (no gradient computation)\n",
    "    - Computing Top-1 accuracy across the entire validation set\n",
    "    - Providing progress tracking during evaluation\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to evaluate (float or quantized)\n",
    "        testloader: DataLoader containing validation dataset\n",
    "        mode: String identifier for logging (e.g., 'Float', 'PTQ_Pytorch')\n",
    "    \n",
    "    Returns:\n",
    "        float: Top-1 accuracy percentage\n",
    "    \"\"\"\n",
    "    # Determine best available device for inference\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Perform inference without gradient computation for efficiency\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testloader):\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass to get predictions\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Calculate and display accuracy\n",
    "    val_acc = (100 * correct / total)\n",
    "    print(mode + ' Accuracy: %.2f%%' % val_acc)\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ae3d59",
   "metadata": {},
   "source": [
    "Create DataLoader for evaluation with larger batch size for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d94e0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(dataset, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e911e35c",
   "metadata": {},
   "source": [
    "Finally, let's evaluate each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original floating-point PyTorch model\n",
    "evaluate(float_model, val_dataloader, 'Float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19492a4c-6ec7-4643-82c0-ce448ba0b8de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PTQ model\n",
    "evaluate(quantized_model_ptq, val_dataloader, 'PTQ_Pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd1bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PTQ + Mixed Precision model\n",
    "evaluate(quantized_model_ptq_mixed_precision, val_dataloader, 'PTQ_Pytorch_mixed_precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae625b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPTQ model\n",
    "evaluate(quantized_model_gptq, val_dataloader, 'GPTQ_Pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c579cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPTQ + Mixed Precision quantized model\n",
    "evaluate(quantized_model_gptq_mixed_precision, val_dataloader, 'GPTQ_Pytorch_mixed_precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37baa56",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we demonstrated how to quantize a pre-trained model using MCTWrapper with a few lines of code.\n",
    "\n",
    "\n",
    "## Copyrights\n",
    "\n",
    "Copyright 2025 Sony Semiconductor Solutions, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "torch4 (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
