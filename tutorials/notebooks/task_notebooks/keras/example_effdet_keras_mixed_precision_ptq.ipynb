{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb4870d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# EfficientDet and Mixed-Precision Post-Training Quantization in Keras using the Model Compression Toolkit(MCT)\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/SonySemiconductorSolutions/mct-model-optimization/blob/main/tutorials/notebooks/task_notebooks/keras/example_effdet_keras_mixed_precision_ptq.ipynb)\n",
    "\n",
    "### Attention\n",
    "\n",
    "The MCT (Model Compression Toolkit) used in this tutorial requires TensorFlow 2.15 or earlier, which are not compatible with the default Google Colab environment (Python 3.12 or later).\n",
    "\n",
    "**If you are running this tutorial on Google Colab, you must change the runtime type to use Python 3.11 before proceeding.**  \n",
    "For detailed instructions, please refer to the [README.md](../../../README.md).\n",
    "\n",
    "## Overview\n",
    "This quick-start guide explains how to use the **Model Compression Toolkit (MCT)** to quantize a EfficientDet model. We will load a pre-trained model and quantize it using the MCT with **Mixed-Precision Post-Training Quantization (PTQ)** .\n",
    "\n",
    "## Summary\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "1. Loading and preprocessing COCO’s dataset.\n",
    "2. Constructing an unlabeled representative dataset.\n",
    "3. Post-Training Quantization using MCT.\n",
    "4. Accuracy evaluation of the floating-point and the quantized models.\n",
    "\n",
    "## efficientdet-pytorch(Dependent External Repository)\n",
    "This tutorial uses a pre-trained pytorch model from the repository linked below, and converts to a pre-trained keras model.  \n",
    "Installation instructions are provided in the **Setup** section.   \n",
    "[efficientdet-pytorch](https://github.com/rwightman/efficientdet-pytorch)\n",
    "\n",
    "### License(efficientdet-pytorch)\n",
    "   Copyright 2020 Ross Wightman\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License.\n",
    "\n",
    "## Additional Code Attribution\n",
    "\n",
    "This tutorial uses custom model conversion code located in the `models/efficientdet/` and `models/utils/` directory.\n",
    "These files facilitate the conversion of PyTorch EfficientDet models to Keras/TensorFlow format for use with MCT.\n",
    "\n",
    "### Source Code Attribution\n",
    "\n",
    "The following files contain code derived from open-source PyTorch implementations:\n",
    "\n",
    "**efficientdet-pytorch**\n",
    "- Source: https://github.com/rwightman/efficientdet-pytorch\n",
    "- License: Apache License 2.0\n",
    "- Files: `effdet_keras.py`, `torch2keras_weights_translation.py`\n",
    "- Modifications: PyTorch layers converted to Keras/TensorFlow equivalents, weight loading adapted for Keras format\n",
    "\n",
    "**pytorch-image-models (timm)**\n",
    "- Source: https://github.com/huggingface/pytorch-image-models\n",
    "- License: Apache License 2.0\n",
    "- Files: `effnet_keras.py`, `effnet_blocks_keras.py`\n",
    "- Modifications: `torch.nn.Module` classes converted to Keras layers\n",
    "\n",
    "### License(efficientdet-pytorch)\n",
    "Please refer to the license section described earlier in this notebook.\n",
    "\n",
    "### License(pytorch-image-models)\n",
    "```\n",
    "   Copyright 2019 Ross Wightman\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License.\n",
    "```\n",
    "\n",
    "For detailed attribution information, see the header comments in each file under `models/`.\n",
    "\n",
    "## Setup  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646df95c",
   "metadata": {},
   "source": [
    "First, install the relevant packages:  \n",
    "This step may take several minutes...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b9ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.15.*\n",
    "!pip install numpy==1.26.4\n",
    "!pip install opencv-python==4.9.0.80\n",
    "!pip install pycocotools==2.0.10\n",
    "\n",
    "# install efficientdet-pytorch(effdet) and dependencies\n",
    "!pip install torch==2.6.0 torchvision==0.21.0\n",
    "!pip install timm==0.9.16\n",
    "!pip install effdet==0.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c3f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "    !pip install model_compression_toolkit\n",
    "!git clone https://github.com//SonySemiconductorSolutions/mct-model-optimization.git temp_mct && mv temp_mct/tutorials/notebooks/task_notebooks/keras/models . && \\rm -rf temp_mct\n",
    "sys.path.insert(0,\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "import model_compression_toolkit as mct\n",
    "from edgemdt_cl.keras import SSDPostProcess\n",
    "from edgemdt_cl.keras.object_detection import ScoreConverter\n",
    "\n",
    "from effdet.config import get_efficientdet_config\n",
    "from effdet.anchors import Anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c766698",
   "metadata": {},
   "source": [
    "### Various Settings\n",
    "Here, you can configure the parameters listed below.  \n",
    "\n",
    "#### Parameter setting\n",
    "- IMG_HEIGHT, IMG_WIDTH  \n",
    "  This parameter allows you to set the size of input images.\n",
    "- SCORE_THR  \n",
    "  This parameter allows you to set the threshold of class score for the Non-Maximum Suppression (NMS) and evaluation.\n",
    "- IOU_THR  \n",
    "  This parameter allows you to set the threshold of iou for the Non-Maximum Suppression (NMS).\n",
    "- CALIB_ITER  \n",
    "  This parameter allows you to set how many samples to use when generating representative data for quantization.\n",
    "- WEIGHTS_COMPRESSION_RATIO  \n",
    "  This parameter allows you to set the quantization ratio based on the weight size of the 8-bit model when using mixed-precision quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f4914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter setting\n",
    "IMG_HEIGHT = 320\n",
    "IMG_WIDTH = 320\n",
    "SCORE_THR = 0.001\n",
    "IOU_THR = 0.50\n",
    "CALIB_ITER = 10\n",
    "WEIGHTS_COMPRESSION_RATIO = 0.85\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c928c12f",
   "metadata": {},
   "source": [
    "Load a pre-trained PyTorch model, and Convert to Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d9885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.efficientdet import EfficientDetKeras\n",
    "model_name = 'tf_efficientdet_lite0'\n",
    "config = get_efficientdet_config(model_name)\n",
    "input_shape = [*config.image_size] + [3]\n",
    "\n",
    "float_model = EfficientDetKeras(config, pretrained_backbone=False).get_model(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436710a6",
   "metadata": {},
   "source": [
    "Next, we add the CustomLayer (edgemdt_cl) **SSDPostProcess** as post-processing.  \n",
    "\n",
    "SSDPostProcess: Decodes EfficientDet inference results from Anchor format to BoundingBox format and Executes the Non-Maximum Suppression to remove overlapping boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e10a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make CustomLayer Instance\n",
    "anchors = tf.constant(Anchors.from_config(config).boxes.detach().cpu().numpy())\n",
    "ssd_pp = SSDPostProcess(anchors, [1, 1, 1, 1], [*config.image_size],\n",
    "                        ScoreConverter.SIGMOID, score_threshold=SCORE_THR, iou_threshold=IOU_THR,\n",
    "                        max_detections=config.max_det_per_image)\n",
    "\n",
    "# Add CustomLayer to model\n",
    "input = tf.keras.layers.Input(shape=input_shape)\n",
    "x_class, x_box = float_model(input)\n",
    "outputs = ssd_pp((x_box, x_class))\n",
    "full_float_model = tf.keras.Model(inputs=input, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af4bac0",
   "metadata": {},
   "source": [
    "The input and output formats of SSDPostProcess are shown below.  \n",
    "For detailed attribution information, see [API Document](https://sonysemiconductorsolutions.github.io/aitrios-edge-mdt-cl/edgemdt_cl/keras.html#SSDPostProcess).\n",
    "\n",
    "Inputs:  \n",
    "&emsp;A list or tuple of:  \n",
    "- rel_codes: Relative codes (encoded offsets).  \n",
    "- scores: Scores or logits.  \n",
    "\n",
    "Returns:  \n",
    "&emsp;'CombinedNonMaxSuppression' named tuple:  \n",
    "- nmsed_boxes: Selected boxes sorted by scores in descending order.\n",
    "- nmsed_scores: Scores corresponding to the selected boxes.\n",
    "- nmsed_classes: Labels corresponding to the selected boxes. \n",
    "- valid_detections: The number of valid detections out of max_detections(unused in this tutorial)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a4d437",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "### Download COCO's dataset\n",
    "\n",
    "**Note**  \n",
    "In this tutorial, we will use a subset of COCO train2017 for calibration during quantization and COCO val2017 for evaluation.\n",
    "\n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe82c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('COCO_dataset'):\n",
    "    !mkdir COCO_dataset\n",
    "    !wget -P COCO_dataset http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "    !wget -P COCO_dataset http://images.cocodataset.org/zips/train2017.zip\n",
    "    !wget -P COCO_dataset http://images.cocodataset.org/zips/val2017.zip\n",
    "    !unzip COCO_dataset/annotations_trainval2017.zip -d COCO_dataset\n",
    "    !unzip COCO_dataset/train2017.zip -d COCO_dataset\n",
    "    !unzip COCO_dataset/val2017.zip -d COCO_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe934651",
   "metadata": {},
   "source": [
    "Here, we are setting the paths for the annotation file and image folder of the downloaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d8913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_TRAIN_IMG_DIR = \"COCO_dataset/train2017/\"\n",
    "COCO_VAL_IMG_DIR = \"COCO_dataset/val2017/\"\n",
    "COCO_TRAIN_ANN_JSON = \"COCO_dataset/annotations/instances_train2017.json\"\n",
    "COCO_VAL_ANN_JSON = \"COCO_dataset/annotations/instances_val2017.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c60b0",
   "metadata": {},
   "source": [
    "In this class, we process the downloaded COCO's dataset for calibration during quantization and for use in evaluation.  \n",
    "We define the dataset and dataloader for COCO's dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9c159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset:\n",
    "    \"\"\"\n",
    "    COCO dataset class like pytorch.\n",
    "    Preprocessor matching the pipeline of EfficientDet(efficientdet-pytorch).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_dir: str, ann_json: str, img_size: Tuple = (320, 320)):\n",
    "        \"\"\"\n",
    "        Initialize COCO dataset.\n",
    "\n",
    "        Args:\n",
    "            img_dir (str): A directory path containing COCO images.\n",
    "            ann_json (str): A file path to COCO annotation json file.\n",
    "            img_size (Tuple[int, int]): Target image size for EfficientDet(efficientdet-pytorch) model.\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.coco = COCO(ann_json)\n",
    "        self.img_ids = self.coco.getImgIds()\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # Normalization parameters matching EfficientDet(efficientdet-pytorch) configuration\n",
    "        self.mean = np.array([0.5, 0.5, 0.5], dtype=np.float32)\n",
    "        self.std = np.array([0.5, 0.5, 0.5], dtype=np.float32)\n",
    "        self.fill_value = (self.mean * 255).astype(np.uint8)\n",
    "\n",
    "    def __len__(self) -> int: \n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Iteration of COCO dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the image in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: A dictionary containing:\n",
    "                'input' (np.ndarray): Preprocessed image.\n",
    "                'id' (int): Image ID.\n",
    "                'file_name' (str): Image file name.\n",
    "                'ratio' (float): Scale factor used in preprocessing.\n",
    "        \"\"\"\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_info = self.coco.loadImgs([img_id])[0]\n",
    "        img_path = os.path.join(self.img_dir, img_info['file_name'])\n",
    "\n",
    "        org_img = cv2.imread(img_path)\n",
    "        org_img = cv2.cvtColor(org_img, cv2.COLOR_BGR2RGB)\n",
    "        input_img, ratio = self.preprocess(input_img=org_img)\n",
    "\n",
    "        sample = {\n",
    "            'input': input_img,\n",
    "            'id': img_id,\n",
    "            'file_name': img_info['file_name'],\n",
    "            'ratio': ratio\n",
    "        }\n",
    "        return sample\n",
    "    \n",
    "    def preprocess(self, input_img: np.ndarray) -> Tuple:\n",
    "        \"\"\"\n",
    "        Preprocess image to match the pipeline of EfficientDet(efficientdet-pytorch).\n",
    "        \n",
    "        Args:\n",
    "            input_img (np.ndarray): Input image in HWC format.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, float]:\n",
    "                - Preprocessed image.\n",
    "                - Scale factor used in resizing.\n",
    "        \"\"\"\n",
    "        height, width = input_img.shape[:2]\n",
    "        target_h, target_w = self.img_size\n",
    "        \n",
    "        # Calculate scale factor for letterbox resize\n",
    "        img_scale = min(target_h / height, target_w / width)\n",
    "        \n",
    "        # Resize with bilinear interpolation\n",
    "        scaled_h = int(height * img_scale)\n",
    "        scaled_w = int(width * img_scale)\n",
    "        resized_img = cv2.resize(input_img, (scaled_w, scaled_h), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # Pad with mean value\n",
    "        padded_img = np.full((target_h, target_w, 3), self.fill_value, dtype=np.uint8)\n",
    "        padded_img[:scaled_h, :scaled_w, :] = resized_img\n",
    "        \n",
    "        # Normalize: (x/255 - mean) / std\n",
    "        normalized_img = (padded_img.astype(np.float32) / 255.0 - self.mean) / self.std\n",
    "\n",
    "        return normalized_img, img_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3171def3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataLoader:\n",
    "    \"\"\"\n",
    "    Dataloader class like pytorch for CocoDataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: List[Tuple], batch_size: int, shuffle: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize COCO dataloader.\n",
    "\n",
    "        Args:\n",
    "            dataset (List[Tuple]): A list of dataset samples.\n",
    "            batch_size (int): Number of samples per batch.\n",
    "            shuffle (bool): Whether to shuffle the dataset at the start of each iteration.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.count = 0\n",
    "        self.inds = list(range(len(dataset)))\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.inds)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __next__(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Iteration of COCO dataloader.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: A dictionary containing:\n",
    "                'input' (np.ndarray): Preprocessed image.\n",
    "                'id' (int): Image ID.\n",
    "                'file_name' (str): Image file name.\n",
    "                'ratio' (float): Scale factor used in preprocessing.\n",
    "        \"\"\"\n",
    "        if self.count >= len(self.dataset):\n",
    "            raise StopIteration\n",
    "\n",
    "        batch_sample = {}\n",
    "        batch_count = 0\n",
    "        while batch_count < self.batch_size and self.count < len(self.dataset):\n",
    "            index = self.inds[self.count]\n",
    "            sample = self.dataset[index]\n",
    "            for sample_key in sample.keys():\n",
    "                batch_sample.setdefault(sample_key, []).append(sample[sample_key])\n",
    "            self.count += 1\n",
    "            batch_count += 1\n",
    "        for sample_key in batch_sample.keys():\n",
    "            batch_sample[sample_key] = np.array(batch_sample[sample_key])\n",
    "\n",
    "        return batch_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4061db7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = CocoDataset(\n",
    "    img_dir = COCO_VAL_IMG_DIR, ann_json = COCO_VAL_ANN_JSON,\n",
    "    img_size = (IMG_HEIGHT, IMG_WIDTH)\n",
    ")\n",
    "train_dataset = CocoDataset(\n",
    "    img_dir = COCO_TRAIN_IMG_DIR, ann_json=COCO_TRAIN_ANN_JSON,\n",
    "    img_size = (IMG_HEIGHT, IMG_WIDTH)\n",
    ")\n",
    "\n",
    "# For evaluation\n",
    "val_dataloader = CocoDataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "# For calibration（No label required）\n",
    "calib_loader = CocoDataLoader(\n",
    "    train_dataset, batch_size=1, shuffle=False\n",
    ")\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2915a6",
   "metadata": {},
   "source": [
    "## Representative Dataset\n",
    "For quantization with MCT, we need to define a representative dataset required by the PTQ algorithm. This dataset is a generator that returns a list of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bba02a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset_gen():\n",
    "    for sample in itertools.islice(itertools.cycle(calib_loader), CALIB_ITER):\n",
    "        yield [sample['input']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea063a",
   "metadata": {},
   "source": [
    "## Target Platform Capabilities (TPC)\n",
    "In addition, MCT optimizes the model for dedicated hardware platforms. This is done using TPC (for more details, please visit our [documentation](https://sonysemiconductorsolutions.github.io/mct-model-optimization/api/api_docs/modules/target_platform_capabilities.html)). Here, we use the TPC object for imx500 hardware with version 1.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f5715",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpc = mct.get_target_platform_capabilities(tpc_version='1.0', device_type='imx500')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d232bb",
   "metadata": {},
   "source": [
    "## Mixed Precision Configurations\n",
    "We will create a `MixedPrecisionQuantizationConfig` that defines the search options for mixed-precision:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25783c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = mct.core.CoreConfig(\n",
    "    mixed_precision_config=mct.core.MixedPrecisionQuantizationConfig(num_of_images=CALIB_ITER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f1557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Resource Utilization information to constraint your model's memory size.\n",
    "resource_utilization_data = mct.core.keras_resource_utilization_data(\n",
    "    full_float_model,\n",
    "    representative_dataset_gen,\n",
    "    configuration,\n",
    "    target_platform_capabilities=tpc)\n",
    " \n",
    "# Define target Resource Utilization for mixed precision weights quantization.\n",
    "resource_utilization = mct.core.ResourceUtilization(resource_utilization_data.weights_memory * WEIGHTS_COMPRESSION_RATIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6fcf4d",
   "metadata": {},
   "source": [
    "# Post-Training Quantization using MCT\n",
    "Now for the exciting part! Let's run PTQ on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31c7f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model, quantization_info = mct.ptq.keras_post_training_quantization(\n",
    "                                        in_model=full_float_model,\n",
    "                                        representative_data_gen=representative_dataset_gen,\n",
    "                                        target_platform_capabilities=tpc,\n",
    "                                        core_config=configuration,\n",
    "                                        target_resource_utilization=resource_utilization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773b178",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "Now, we will create a function for evaluating a model.  \n",
    "The inference results before and after quantization are displayed on the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d0e32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def inference_step(model: tf.keras.Model, \n",
    "                   input_imgs: np.ndarray) -> Any:\n",
    "    \"\"\"\n",
    "    Model Inference Wrapper for @tf.function.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): Evaluation model.\n",
    "        input_imgs (np.ndarray): Input image.\n",
    "    \n",
    "    Returns:\n",
    "        Any: Model outputs.\n",
    "    \"\"\"\n",
    "    return model(input_imgs, training=False)\n",
    "\n",
    "def evaluate(model: tf.keras.Model, val_dataloader: CocoDataLoader,\n",
    "             score_threshold: float = 0.1):\n",
    "    \"\"\"\n",
    "    Evaluation of the COCO dataset.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): Evaluation model.\n",
    "        val_dataloader (CocoDataLoader): Evaluation dataset.\n",
    "        score_threshold (float): Score threshold.\n",
    "    \"\"\"\n",
    "    model.trainable = False\n",
    "\n",
    "    results = []\n",
    "    for sample in tqdm(val_dataloader, desc=\"Evaluating\"):\n",
    "        input_imgs = sample['input']\n",
    "        img_ids = sample['id']\n",
    "        ratios = sample['ratio']\n",
    "\n",
    "        nmsed_boxes, nmsed_scores, nmsed_classes, _ = inference_step(model, input_imgs)\n",
    "        nmsed_boxes = nmsed_boxes.numpy()\n",
    "        nmsed_scores = nmsed_scores.numpy()\n",
    "        nmsed_classes = nmsed_classes.numpy()\n",
    "\n",
    "        for batch_idx in range(len(img_ids)):\n",
    "            img_id = img_ids[batch_idx]\n",
    "            ratio = ratios[batch_idx]\n",
    "            # boxes: [N, 4] (ymin, xmin, ymax, xmax), scores: [N], labels: [N]\n",
    "            for box, score, label in zip(nmsed_boxes[batch_idx], nmsed_scores[batch_idx], nmsed_classes[batch_idx]):\n",
    "                if score > score_threshold:\n",
    "                    box /= ratio\n",
    "                    y_min, x_min, y_max, x_max = box.tolist()\n",
    "                    width = x_max - x_min\n",
    "                    height = y_max - y_min\n",
    "                    result = {\n",
    "                        'image_id': int(img_id),\n",
    "                        'category_id': int(label) + 1,  # Convert class index (0-based) to COCO category ID (1-based)\n",
    "                        'bbox': [int(x_min), int(y_min), int(width), int(height)],\n",
    "                        'score': float(score),\n",
    "                    }\n",
    "                    results.append(result)\n",
    "\n",
    "    # evaluation\n",
    "    coco_gt = val_dataset.coco\n",
    "\n",
    "    coco_dt = coco_gt.loadRes(results)\n",
    "    evaluator = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "    evaluator.evaluate()\n",
    "    evaluator.accumulate()\n",
    "    evaluator.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029eb053",
   "metadata": {},
   "source": [
    "Let's start with the floating-point model evaluation.  \n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a557583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"evaluating float model（COCO mAP）...\")\n",
    "evaluate(full_float_model, val_dataloader,\n",
    "         score_threshold = SCORE_THR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ef5d0b",
   "metadata": {},
   "source": [
    "Finally, let's evaluate the quantized model:  \n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba05924",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"evaluating quantized model（COCO mAP）...\")\n",
    "evaluate(quantized_model, val_dataloader,\n",
    "         score_threshold = SCORE_THR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b590922",
   "metadata": {},
   "source": [
    "## Export and Load the quantized model\n",
    "Lastly, we will demonstrate how to export the quantized model into a file and then load it.\n",
    "\n",
    "We will use `keras_export_model` function to save the quantized model with the integrated custom quantizers into a \".keras\" file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c42212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export a keras model with mctq custom quantizers into a file\n",
    "mct.exporter.keras_export_model(model=quantized_model,\n",
    "                                save_model_path='./effdet_keras_mixed_precision_ptq.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac971ae",
   "metadata": {},
   "source": [
    "Then, we can load the saved model using `keras_load_quantized_model` function. For this specific case, we'll have to supply the load function with an extra custom layer integrated into the model, namely `SSDPostProcess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613e3095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from edgemdt_cl.keras.object_detection.ssd_post_process import SSDPostProcess\n",
    "\n",
    "custom_objects = {SSDPostProcess.__name__: SSDPostProcess} # An extra custom layer integrated in the model \n",
    "quant_model_from_file = mct.keras_load_quantized_model('./effdet_keras_mixed_precision_ptq.keras', custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1256f29a",
   "metadata": {},
   "source": [
    "## Copyrights\n",
    "\n",
    "Copyright 2025 Sony Semiconductor Solutions, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras12 (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
